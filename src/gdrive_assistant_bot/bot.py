import logging

from openai import OpenAI
from telegram import Update
from telegram.ext import Application, CommandHandler, ContextTypes, MessageHandler, filters

from .health import start_health_server
from .logging import setup_logging
from .rag import RAGStore
from .settings import settings

_MIN_CMD_PARTS = 2

log = logging.getLogger("gdrive-assistant-bot.bot")


def _make_llm_client() -> OpenAI | None:
    if not settings.llm_enabled():
        return None
    return OpenAI(base_url=settings.LLM_BASE_URL, api_key=settings.LLM_API_KEY)


def _truncate_text(text: str, max_chars: int = 4000) -> str:
    return text[:max_chars] + "‚Ä¶\n\n‚Ä¶(–∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ–±—Ä–µ–∑–∞–Ω–∞)" if len(text) > max_chars else text


async def cmd_start(update: Update, _: ContextTypes.DEFAULT_TYPE) -> None:
    await update.message.reply_text(
        "ü§ñ –Ø ‚Äî –±–æ—Ç-–∞—Å—Å–∏—Å—Ç–µ–Ω—Ç –¥–ª—è Google Drive.\n\n"
        "–ö–æ–º–∞–Ω–¥—ã:\n\n"
        "‚Äì /ask <–≤–æ–ø—Ä–æ—Å> ‚Äî –Ω–∞–π—Ç–∏ –æ—Ç–≤–µ—Ç –≤ –±–∞–∑–µ –∑–Ω–∞–Ω–∏–π\n"
        "‚Äì /ingest <—Ç–µ–∫—Å—Ç> ‚Äî –¥–æ–±–∞–≤–∏—Ç—å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –≤ –±–∞–∑—É –∑–Ω–∞–Ω–∏–π\n"
    )


async def cmd_ingest(update: Update, context: ContextTypes.DEFAULT_TYPE) -> None:
    msg = update.message
    if not msg or not msg.text:
        return

    parts = msg.text.split(maxsplit=1)
    if len(parts) < _MIN_CMD_PARTS or not parts[1].strip():
        await msg.reply_text("–ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ: /ingest <—Ç–µ–∫—Å—Ç>")
        return

    store: RAGStore = context.application.bot_data["store"]

    text = parts[1].strip()
    doc_id = f"telegram:{msg.chat_id}:{msg.message_id}"

    payload = {
        "file_id": doc_id,
        "file_name": "telegram_message",
        "file_type": "telegram",
        "modified_time": str(msg.date),
        "from_user": (msg.from_user.username if msg.from_user else None),
        "chat_id": str(msg.chat_id),
        "message_id": msg.message_id,
    }

    n = store.upsert_document(
        doc_id=doc_id, source=f"telegram:{msg.chat_id}", text=text, payload=payload
    )
    await msg.reply_text(f"–ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –¥–æ–±–∞–≤–ª–µ–Ω–∞ –≤ –±–∞–∑—É –∑–Ω–∞–Ω–∏–π ({n} —á–∞—Å—Ç–µ–π)")


async def cmd_ask(update: Update, context: ContextTypes.DEFAULT_TYPE) -> None:
    msg = update.message
    if not msg or not msg.text:
        return

    parts = msg.text.split(maxsplit=1)
    if len(parts) < _MIN_CMD_PARTS or not parts[1].strip():
        await msg.reply_text("–ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ: /ask <–≤–æ–ø—Ä–æ—Å>")
        return

    store: RAGStore = context.application.bot_data["store"]
    llm: OpenAI | None = context.application.bot_data["llm"]

    question = parts[1].strip()

    hits = store.search(question)
    context_text = store.build_context(hits, max_chars=settings.MAX_CONTEXT_CHARS)

    if not context_text.strip():
        await msg.reply_text("–ù–∏—á–µ–≥–æ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ")
        return

    if not llm:
        preview = _truncate_text(context_text)
        await msg.reply_text("–Ø–∑—ã–∫–æ–≤–∞—è –º–æ–¥–µ–ª—å –Ω–µ –Ω–∞—Å—Ç—Ä–æ–µ–Ω–∞. –ù–∞–π–¥–µ–Ω–Ω—ã–µ —Ñ—Ä–∞–≥–º–µ–Ω—Ç—ã:\n\n" + preview)
        return

    prompt = f"–ö–æ–Ω—Ç–µ–∫—Å—Ç:\n\n{context_text}\n\n–í–æ–ø—Ä–æ—Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è:\n\n{question}"

    try:
        resp = llm.chat.completions.create(
            model=settings.LLM_MODEL,
            messages=[
                {"role": "system", "content": settings.LLM_SYSTEM_PROMPT},
                {"role": "user", "content": prompt},
            ],
        )
        answer = _truncate_text((resp.choices[0].message.content or "").strip()) or "–ü—É—Å—Ç–æ–π –æ—Ç–≤–µ—Ç"
        await msg.reply_text(answer)
    except Exception:
        log.exception("llm_call_failed", extra={"component": "bot", "event": "llm_failed"})
        await msg.reply_text("–û—à–∏–±–∫–∞ —è–∑—ã–∫–æ–≤–æ–π –º–æ–¥–µ–ª–∏")


async def on_plain_text(update: Update, context: ContextTypes.DEFAULT_TYPE) -> None:
    await cmd_start(update, context)


def main() -> None:
    setup_logging()
    start_health_server(settings.HEALTH_HOST, settings.BOT_HEALTH_PORT, component="bot")

    log.info("startup", extra={"component": "bot", "event": "startup"})
    log.info("config", extra={"component": "bot", "event": "config", "count": settings.safe_dump()})

    store = RAGStore()
    llm = _make_llm_client()

    app = Application.builder().token(settings.TELEGRAM_BOT_TOKEN).build()
    app.bot_data["store"] = store
    app.bot_data["llm"] = llm

    app.add_handler(CommandHandler("start", cmd_start))
    app.add_handler(CommandHandler("ingest", cmd_ingest))
    app.add_handler(CommandHandler("ask", cmd_ask))
    app.add_handler(MessageHandler(filters.TEXT & ~filters.COMMAND, on_plain_text))

    log.info("polling", extra={"component": "bot", "event": "polling"})
    app.run_polling(close_loop=False)


if __name__ == "__main__":
    main()
